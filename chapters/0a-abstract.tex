\chapter*{Abstract}

\begin{comment}
A condensation of the essential information of the article.
\end{comment}

In today's increasingly autonomous society, the demand for intelligent, robust solutions are pushing research within guidance, navigation and control to new limits.
In light of its recent advancements, reinforcement learning has established itself as an compelling candidate for the task of guidance and control -- by achieving state-of-the-art performances in robotic tasks. 
Since then, a multitude of algorithms have emerged in this field which has prompted a natural question: how do we decide which algorithm is best for a certain robotic task?
This project aims to serve as a starting point for this question, comparing the characteristics and performances of the two popular reinforcement learning algorithms, Deep Deterministic Policy (DDPG) Gradients and Proximal Policy Optimisation (PPO). 
In this project, these algorithms are trained in a guidance task, commanding accelerations to reach a certain waypoint in 3-dimensional space. Furthermore, models with different hyperparameters combinations are trained and tested, and the results of each analysed. From this, it was discovered that DDPG offered high stability, and by increasing its learning rates and reducing the size of its replay buffer we could improve results. The training of PPO was unexpectedly difficult, where only one model converged to an optimal performance while the rest diverged. The reasons for this are uncertain, though likely caused by too fast learning in the PPO models. Nonetheless, by comparing models relative to a local optimum, it was found that by increasing the batch size, one could then increase the number of optimisation epochs, the number of minibatches and value coefficient to speed up the learning process, while achieving roughly the same average performance. Last, the entropy coefficient was beneficial offering extra exploration. Yet, as the models diverged, these results provide an in-depth understanding of the hyperparameters, but is in no regard a recommendation. Finally, the robustness of the algorithms were tested with a $\pm10\%$ change in mass to the quadrotor. This proved to be an difficult task for the models, where only the best PPO model was able to succeed. Thus, we conclude that the DDPG algorithm is easier to train than PPO, though the potential performance of the PPO algorithm is higher.